{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "\n",
    "Here is the rewritten text without links:\n",
    "\n",
    "AI Interpretability\n",
    "\n",
    "Table of Contents\n",
    "Summary\n",
    "History\n",
    "Importance\n",
    "Regulatory Compliance\n",
    "Trust and Acceptance\n",
    "Feature Importance\n",
    "Ethical Implications\n",
    "User Trust and Ethical AI\n",
    "Methods\n",
    "Direct Interpretability and XAI Perspectives\n",
    "Simulation Design\n",
    "Advances in Interpretability\n",
    "Application to the Simulation\n",
    "Applications\n",
    "Healthcare\n",
    "Finance\n",
    "Manufacturing\n",
    "Digital Twins\n",
    "Anomaly Detection\n",
    "Challenges\n",
    "Ethical Concerns\n",
    "Ethical Implications of Explanations\n",
    "Trust and Accountability\n",
    "Transparency and Ethical Integrity\n",
    "Potential Impacts of Neglecting Transparency\n",
    "Future Directions\n",
    "\n",
    "Summary\n",
    "AI interpretability refers to the ability of artificial intelligence (AI) systems to explain their decision-making processes and outcomes in a manner that is understandable to humans. This field has gained significant traction over several decades, evolving from early rule-based systems like MYCIN and GUIDON, to more advanced methodologies involving neural networks and visualization tools. The primary aim of AI interpretability is to foster trust and transparency in AI systems by making their operations more accessible and understandable. Notable advancements include the development of \"explainable by design\" models and techniques such as Integrated Gradients and feature importance plots.\n",
    "\n",
    "The importance of AI interpretability spans several key domains including regulatory compliance, trust and acceptance, feature importance, and ethical considerations. With AI systems increasingly being deployed in critical areas such as healthcare, finance, and criminal justice, the ability to explain AI decisions is essential for meeting legal standards, building user trust, and ensuring ethical integrity. Transparent AI models help organizations comply with regulations and foster greater public acceptance by elucidating the rationale behind their decisions.\n",
    "\n",
    "Various methods are employed to achieve AI interpretability, ranging from direct interpretability models like decision trees and linear models, to explainable AI (XAI) techniques that provide post-hoc explanations for complex models. Advances in this field include the development of tools that offer both global and local explanations, helping users understand both the overall behavior of the model and specific decisions it makes. Effective visualization techniques play a crucial role in this context, aiding in the clear communication of AI decisions to stakeholders.\n",
    "\n",
    "Despite its benefits, AI interpretability faces several challenges including balancing model performance with transparency, ensuring the scalability of version control mechanisms, and addressing social and ethical considerations. The quest for interpretable AI also involves navigating complex ethical landscapes, where transparency and fairness are paramount. As the field continues to evolve, ongoing research aims to enhance the reliability and applicability of interpretability methods, ensuring they meet the diverse needs of various industries and stakeholders.\n",
    "\n",
    "History\n",
    "The study of AI interpretability has a rich history that spans several decades. Early work in this area focused on the development of systems capable of providing explanations for their decisions. Notable among these early systems was MYCIN, a computer-based medical decision-making system developed in the 1970s, which aimed to offer explanations for its diagnostic conclusions using rule-based logic.\n",
    "\n",
    "In the 1980s, the focus shifted towards more sophisticated pedagogical and knowledge engineering techniques. The SOPHIE systems (I, II, and III) exemplified this shift, employing natural language and pedagogical techniques to enhance their explanatory capabilities. This era also saw the emergence of exemplar-based learning systems, such as PROTOS, which utilized examples to facilitate knowledge acquisition and explanation.\n",
    "\n",
    "The 1990s and early 2000s brought further advancements with systems that integrated various AI methodologies to improve interpretability. For example, the PROTOS system and other exemplar-based learning apprentices demonstrated the utility of examples in explaining AI decisions. The development of explainable artificial intelligence (XAI) systems for tactical behavior in small units by Van Lent and colleagues is another key milestone from this period.\n",
    "\n",
    "Recent years have seen a surge in research dedicated to developing more transparent and interpretable AI models. The rise of \"explainable by design\" methods, such as linear models, decision trees, and Bayesian models, has been particularly notable. These methods offer inherent transparency, making it easier to understand the decision-making processes of AI systems.\n",
    "\n",
    "Importance\n",
    "The importance of AI interpretability is multifaceted, impacting trust, regulatory compliance, ethical considerations, and the overall effectiveness of AI systems.\n",
    "\n",
    "Regulatory Compliance\n",
    "As AI systems become more prevalent in decision-making processes, regulatory compliance becomes a growing concern. Explainable AI (XAI) helps ensure that AI systems adhere to legal and ethical standards by providing the necessary documentation and rationale for decisions to satisfy regulatory requirements. This transparency builds trust with users and regulators while protecting the company's competitive advantage and complying with privacy laws.\n",
    "\n",
    "Trust and Acceptance\n",
    "Trust is a critical factor for the adoption and deployment of AI systems. Humans are more likely to accept algorithmic decisions when they understand how those decisions are made. Interpretability allows users to see the rationale behind a model's output, which can increase confidence and trust in the system.\n",
    "\n",
    "Feature Importance\n",
    "Feature importance is a technique used to understand which input features are most influential in determining the output of a machine learning model. This method helps to reveal the inner workings of an AI system, contributing to its interpretability.\n",
    "\n",
    "Ethical Implications\n",
    "The ethical landscape of AI interpretability is complex. Transparent and interpretable AI systems are essential for ensuring that AI operations are unbiased and fair, making just decisions across all demographics. This fairness is crucial for building an ethical foundation in AI development.\n",
    "\n",
    "User Trust and Ethical AI\n",
    "Consumers increasingly demand transparency in AI systems, especially when these systems have significant impacts on their lives, such as in loan approvals or job applications. According to a survey, a majority of respondents indicated higher trust, loyalty, and willingness to purchase from companies they perceived as practicing ethical AI.\n",
    "\n",
    "Methods\n",
    "Direct Interpretability and XAI Perspectives\n",
    "Interpretability in artificial intelligence (AI) can be approached from two main perspectives: direct interpretability and explainable AI (XAI). Direct interpretability focuses on creating models that are inherently understandable, while XAI aims to develop techniques that explain the decisions of more complex, often opaque models.\n",
    "\n",
    "Simulation Design\n",
    "We relate interpretability approaches to data science principles, with interpretable models exemplifying parsimony and simulatability. Parsimony refers to the ability to trace predictions back to a few key model components, each with an understandable story. Simulatability formalizes the idea that predictions can be manually reconstructed from the model's description, as seen in decision trees and falling rule lists, which can be navigated through a series of yes-no questions.\n",
    "\n",
    "Advances in Interpretability\n",
    "The question of what makes an algorithm interpretable is complex. However, we can consider what makes data visualization effective to gain insight. Effective visualizations streamline cognitive operations into perceptual ones, using familiar or easily learnable representations and well-annotated graphical elements. Information-dense visualizations prevent oversummarization and highlight details for further study. Similarly, interpretable models can be broken down into relevant components with assignable meanings.\n",
    "\n",
    "Application to the Simulation\n",
    "In contrast to deep learning models optimized through a single problem, decision trees are defined through a recursive algorithm. Initially, thresholds maximizing classification accuracy are determined, defining the first partition along the chosen axis.\n",
    "\n",
    "Applications\n",
    "Explainable Artificial Intelligence (XAI) has numerous applications across different industries, each benefiting from the increased transparency and trust it brings to AI systems.\n",
    "\n",
    "Healthcare\n",
    "In the healthcare industry, XAI is crucial for diagnostics, preventative care, and administrative tasks. Hospitals use explainable AI for cancer detection and treatment, providing reasoning behind a given model's decision-making. This helps doctors make informed treatment decisions and offers patients data-backed explanations, increasing trust in AI-assisted healthcare solutions.\n",
    "\n",
    "Finance\n",
    "Explainable AI is essential in the finance industry due to its heavily regulated nature. It helps hold AI models accountable, ensuring fairness and transparency in credit scoring, insurance claim assessment, investment portfolio management, and more.\n",
    "\n",
    "Manufacturing\n",
    "The XMANAI platform serves as a comprehensive solution for explainable AI in the manufacturing sector. It has been validated through four core use cases in the automotive, white goods, machinery, and metrology industries, utilizing innovative manufacturing applications and services.\n",
    "\n",
    "Digital Twins\n",
    "In the context of Digital Twins (DT), explainable and interpretable AI is used to enhance the transparency of AI systems' decision-making processes. Interpretable AI allows users to understand and justify the AI systems' judgments and predictions.\n",
    "\n",
    "Anomaly Detection\n",
    "Explainable AI plays a significant role in anomaly detection across various fields. For instance, vibration signals analysis by explainable AI approaches has been applied to bearing fault diagnosis, where XAI models provide insights into the factors leading to the detection of faults.\n",
    "\n",
    "Challenges\n",
    "AI interpretability poses several challenges that hinder the full realization of transparent and accountable AI systems. One significant challenge is obtaining reliable and complete operational data across diverse assets, which is crucial for training accurate models.\n",
    "\n",
    "Ethical Concerns\n",
    "The discourse on the moral and ethical implications of advanced AI systems underscores the need for continuous exploration and dialogue to navigate evolving ethical challenges. Strengthening the ethical foundation of AI demands interdisciplinary collaboration across technology, humanities, and social sciences.\n",
    "\n",
    "Ethical Implications of Explanations\n",
    "Embedding complex human values and ethics in AI systems is a profound challenge, acknowledging the subjective and culturally dependent nature of these concepts. Developing ethically aligned AI requires a deep understanding of diverse cultural and moral frameworks. Responsible AI necessitates transparent systems that can clearly articulate their reasoning to build user trust in ethical decision-making.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fpdf\n",
    "\n",
    "# Create a PDF object\n",
    "pdf = fpdf.FPDF()\n",
    "\n",
    "# Add a page\n",
    "pdf.add_page()\n",
    "\n",
    "# Set the font\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "# Add the text data\n",
    "# text = \"Your long text here...\"\n",
    "pdf.multi_cell(0, 5, txt=text)\n",
    "\n",
    "# Save the PDF to a file\n",
    "pdf.output(\"output.pdf\", \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fpdf\n",
    "\n",
    "# Create a PDF object\n",
    "pdf = fpdf.FPDF()\n",
    "\n",
    "# Add a page\n",
    "pdf.add_page()\n",
    "\n",
    "# Set the font\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "# Add the text data\n",
    "pdf.cell(0, 10, txt=text, ln=True)\n",
    "\n",
    "# Save the PDF to a file\n",
    "pdf.output(\"output.pdf\", \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String contains [FILE]\n",
      "String contains [EMAIL]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def check_for_file(s):\n",
    "    pattern = r'\\[FILE\\]'\n",
    "    if re.search(pattern, s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_for_email(s):\n",
    "    pattern = r'\\[EMAIL\\]'\n",
    "    if re.search(pattern, s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Example usage:\n",
    "s = \"This is a string with [FILE] inside\"\n",
    "if check_for_file(s):\n",
    "    print(\"String contains [FILE]\")\n",
    "\n",
    "s = \"This is a string with [EMAIL] inside\"\n",
    "if check_for_email(s):\n",
    "    print(\"String contains [EMAIL]\")\n",
    "\n",
    "s = \"This is a string without [FILE] or [EMAIL]\"\n",
    "if not check_for_file(s) and not check_for_email(s):\n",
    "    print(\"String does not contain [FILE] or [EMAIL]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
